<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Openstory++">
  <meta name="keywords" content="Openstory++">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Openstory++ : A Large-scale Dataset and Benchmark for Instance-aware Open-domain
    Visual Storytelling</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/openstorypp_image/pipeline.png">


  <style>
    table {
      font-family: arial, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }

    td,
    th {
      border: 2px solid #F1F4F5;
      text-align: left;
      padding: 8px;
    }

    tr:nth-child(3n - 1) {
      background-color: #F1F4F5;
    }

    tr:nth-child(3n) {
      border: 2px solid #FFFFFF;
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <font color="black">Openstory++ : A Large-scale Dataset and Benchmark for Instance-aware Open-domain
                Visual Storytelling</font>
            </h1>
            <!-- <div class="is-size-5 publication-authors"> -->
            <!-- <span class="author-block"> -->
            <!-- <a href="https://www.linkedin.com/in/levonkhachatryan">Levon Khachatryan</a><sup>1*</sup>,</span> -->
            <!-- <span class="author-block">
              <a href="https://www.linkedin.com/in/andranik-movsisyan-4528a51a2">Andranik Movsisyan</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/vtadevosian">Vahram Tadevosyan</a><sup>1*</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/dr-ing-roberto-henschel-6aa1ba176">Roberto Henschel</a><sup>1*</sup>,</span>
            </span><br>
            <span class="author-block">
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Zhangyang Wang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shant-navasardyan-1302aa149">Shant Navasardyan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.humphreyshi.com">Humphrey Shi</a><sup>1,3,4,5</sup>
            </span> -->
            <!-- </div> -->


            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/YeLuoSuiYou/openstorypp" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/MAPLE-WestLake-AIGC/OpenstoryPlusPlus"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="./static/hf.png" alt="Button Image">
                    </span>
                    <span>Dataset</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/openstorypp_image/figure1.png"
          style="width:100%;height:540px;">
        <p class="subtitle has-text-centered">
          The visualization of our dataset. On the left is a data case with visual annotation that corresponds to each
          entity word in the sentence, where different color stands for different instance visual annotations, and on
          the right is the general pipeline of our dataset annotation process.
        </p>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Recent image generation models excel at creating high-quality images from brief captions. However, they
              fail to maintain consistency of multiple instances across images when encountering lengthy contexts. This
              inconsistency is largely due to in existing training datasets the absence of granular instance feature
              labeling in existing training datasets. To tackle these issues, we introduce <b>Openstory++</b>, a large
              scale dataset combining additional instance-level annotations with both images and text.
              This dataset can be utilized to train multi-modal generated models, allowing for the training of
              instance-focused story visualization models.
              Furthermore, we develop a tailored training methodology that emphasizes entity-centric image-text
              generation, ensuring that the models learn to effectively interweave visual and textual information.
              Specifically, Openstory++ streamlines the process of keyframe extraction from open-domain videos,
              employing vision-language models to generate captions that are then polished by a large language model for
              narrative continuity. It surpasses previous datasets by offering a more expansive open-domain resource,
              which incorporates automated captioning, high-resolution imagery tailored for instance count, and
              extensive frame sequences for temporal consistency. Additionally, we present <b>Cohere-Bench</b>, a
              pioneering benchmark framework for evaluating the image generation tasks when long multimodal context is
              provided, including the ability to keep the background, style, instances in the given context coherent.
              Compared to existing benchmarks, our work fills critical gaps in multi-modal generation, propelling the
              development of models that can adeptly generate and interpret complex narratives in open-domain
              environments. Experiments conducted within Cohere-Bench confirm the superiority of Openstory++ in
              nurturing high-quality visual storytelling models, enhancing their ability to address sophisticated and
              open-domain generation tasks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->

      <!-- <video id="teaser" autoplay muted loop playsinline height="100%"> -->
      <!-- <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 height="100%">
      <source src="./static/videos/main_video_compressed.mp4"
              type="video/mp4">
    </video> -->

      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section" id="Method">
    <div class="container is-max-desktop content">
      <h2 class="title">Method</h2>
      <section class="hero method">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <img id="method" autoplay muted loop playsinline height="100%" src="./static/openstorypp_image/image.png"
              style="width:100%;height:100%;">
            <p>This figure showcases the workflow of our pipeline. After obtaining a sequence of frames devoid of
              redundancy, we first utilized BLIP2 to generate basic image captions. Subsequently, Video-LLaVA was
              employed to produce a sequence of captions that encapsulate the narrative flow. Guided by the sequence
              caption, a LLM was prompted to align the entities in the image captions, thus enhancing the narrative
              coherence across consecutive frames. Next, YOLO-World was applied to detect bounding boxes for the
              entities. To ensure that labels for the same entities across frames are unique and consistent, we blended
              the bounding box labels with the assistance of Dino and a facial feature module. Finally, we employed
              EfficientVIT-SAM to obtain the masks for the entities, thereby providing a comprehensive understanding of
              the spatial extent and characteristics of each entity within the frames.</p>
          </div>
        </div>
        <!-- <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/maple_image/fig2.png"
            style="width:100%;height:100%;">
          <p> Overview of the interleaved image-text generation: Both the image and text are produced
            by the MLLM. During image generation, we have enhanced the system with a visual detokenizer
            based on the diffusion model </p>
        </div> -->
      </section>
    </div>
  </section>





  <!-- <section class="section" id='RelatedLinks'>
  <div class="container is-max-desktop content">
    <h2 class="title">Related Links</h2>

    <ul>
      <li><a href="https://ommer-lab.com/research/latent-diffusion-models/"> High-Resolution Image Synthesis with Latent Diffusion Models (a.k.a. LDM & Stable Diffusion)</a></li>
      <li><a href="https://www.timothybrooks.com/instruct-pix2pix/"> InstructPix2Pix: Learning to Follow Image Editing Instructions</a></li>
      <li><a href="https://github.com/lllyasviel/ControlNet"> Adding Conditional Control to Text-to-Image Diffusion Models (a.k.a ControlNet)</a></li>
    </ul>
    <!-- <div class="content has-text-justified">
      <p>
        There's a lot of excellent work that was introduced around the same time as ours.
      </p>
      <p>
        <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
      </p>
      <p>
        <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
        both use deformation fields to model non-rigid scenes.
      </p>
      <p>
        Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
      </p>
      <p>
        There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
      </p>
    </div> -->
  </div>
  </section>
  </div>
  </section> -->


  <!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p> If you use our work in your research, please cite our publication: </p>
    <pre><code>@article{text2video-zero,
    title={Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators},
    author={Khachatryan, Levon and Movsisyan, Andranik and Tadevosyan, Vahram and Henschel, Roberto and Wang, Zhangyang and Navasardyan, Shant and Shi, Humphrey},
    journal={arXiv preprint arXiv:2303.13439},
    year={2023}
  }
    </code></pre>
  </div>
</section> -->
  <!-- 
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://arxiv.org/pdf/2303.13439.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Picsart-AI-Research/Text2Video-Zero" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>

</html>
